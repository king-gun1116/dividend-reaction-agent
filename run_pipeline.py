# run_pipeline.py  (rev 2025-07-10)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# End-to-End ë°°ë‹¹ ê³µì‹œ Agent & ML íŒŒì´í”„ë¼ì¸
#   1. ë°°ë‹¹ ê³µì‹œ ì¦ë¶„ ìˆ˜ì§‘ (DART)
#   2. ML í•™ìŠµìš© ì •ì œ â†’ dividend_ml_ready.csv
#   2-1. ì£¼ê°€ ìˆ˜ì§‘ + Â±30ì¼ ìœˆë„ìš° ê²€ì¦
#   3. ê³µí†µ í”¼ì²˜ ìƒì„± & ëª¨ë“ˆë³„ ë¶„í•  (classification / regression / clustering)
#   4. ë¬¸ì„œ ì„ë² ë”© & FAISS ì¸ë±ìŠ¤ êµ¬ì¶•
#   5. Notebook ê¸°ë°˜ ëª¨ë¸ í•™ìŠµ (04~06)  â¯ papermill ì‹¤í–‰
#   6. ì•™ìƒë¸” & Master CSV ìƒì„± (07_ensemble.ipynb or inline function)
#   7. ì¶”ê°€ í›„ì²˜ë¦¬ ë…¸íŠ¸ë¶(08_dividend.ipynb) â€“ ì„ íƒì  ì‹¤í–‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

from __future__ import annotations

import os
import sys
import traceback
from datetime import datetime
from typing import List

import pandas as pd
import papermill as pm
from dotenv import load_dotenv

# â”€â”€ ë‚´ë¶€ ìœ í‹¸
from utils.dart_api import collect_dividend_filings_incremental
from utils.data_cleaning import clean_ml_data
from utils.price_fetcher import run_price_fetching
from utils import embed_utils

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Helper: ì•™ìƒë¸” & Master CSV ë¹Œë”
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _build_master_csv(
    module_dir: str,
    data_dir: str,
    master_csv_path: str,
    n_clusters: int = 4,
) -> None:
    """classificationã†regression ê²°ê³¼ë¥¼ í†µí•©í•˜ì—¬ Master CSV ìƒì„±

    Parameters
    ----------
    module_dir       : str  â€“ module_datasets ë””ë ‰í† ë¦¬ (classification / regression csv ìœ„ì¹˜)
    data_dir         : str  â€“ í”„ë¡œì íŠ¸ ìµœìƒìœ„ data ë””ë ‰í† ë¦¬
    master_csv_path  : str  â€“ ìµœì¢… ì €ì¥ ê²½ë¡œ
    n_clusters       : int  â€“ K-Means í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ (default=4)
    """
    from sklearn.preprocessing import StandardScaler
    from sklearn.cluster import KMeans
    import joblib

    # â”€â”€ íŒŒì¼ ê²½ë¡œ
    reg_data_fp  = os.path.join(module_dir, "regression_enriched.csv")
    clf_data_fp  = os.path.join(module_dir, "classification_with_text.csv")
    pred_fp      = os.path.join(
        data_dir,
        "results",
        "regression",
        "regression_predictions_for_ensemble.csv",
    )
    clf_model_fp = os.path.join(data_dir, "models", "lgbm_classifier.pkl")

    # â”€â”€ ë°ì´í„° ë¡œë“œ
    df_reg  = pd.read_csv(reg_data_fp, parse_dates=["rcept_dt"], dtype={"stock_code": str})
    df_clf  = pd.read_csv(clf_data_fp, parse_dates=["rcept_dt"], dtype={"stock_code": str})
    df_pred = pd.read_csv(pred_fp,   parse_dates=["rcept_dt"], dtype={"stock_code": str})

    # â”€â”€ ë¶„ë¥˜ í™•ë¥ (p_up) ê³„ì‚°
    clf_model = joblib.load(clf_model_fp)
    X_clf     = df_clf.drop(columns=[
        "up_1d", "corp_name", "stock_code", "rcept_dt"], errors="ignore"
    )
    df_clf["p_up"] = clf_model.predict_proba(X_clf)[:, 1]

    # â”€â”€ íšŒê·€ residual + y_pred ë¥¼ ì´ìš©í•œ í´ëŸ¬ìŠ¤í„° ë¼ë²¨ ìƒì„±
    scaler        = StandardScaler()
    scaled        = scaler.fit_transform(df_pred[["y_pred", "residual"]])
    df_pred["cluster"] = (
        KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        .fit_predict(scaled)
        .astype("int8")
    )

    # â”€â”€ ë§ˆìŠ¤í„° ë³‘í•©
    df_master = (
        df_pred
        .merge(
            df_clf[["stock_code", "rcept_dt", "p_up"]],
            on=["stock_code", "rcept_dt"],
            how="left",
        )
        .merge(
            df_reg,
            on=["stock_code", "rcept_dt"],
            how="left",
            suffixes=("", "_orig"),
        )
    )

    # â”€â”€ ì €ì¥
    df_master.to_csv(master_csv_path, index=False, encoding="utf-8-sig")
    print(f"âœ… Master CSV saved â†’ {master_csv_path}  (rows: {len(df_master):,})")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Main pipeline function
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def run_pipeline(
    start_date: str = "20130101",
    end_date:   str = datetime.today().strftime("%Y%m%d"),
    data_dir:   str = "data",
    max_workers: int = 10,
    skip_notebooks: List[str] | None = None,
) -> None:
    """ë°°ë‹¹ ê³µì‹œ Agent ì „ì²´ íŒŒì´í”„ë¼ì¸

    Parameters
    ----------
    start_date     : str   â€“ DART ì¡°íšŒ ì‹œì‘ì¼ (YYYYMMDD)
    end_date       : str   â€“ DART ì¡°íšŒ ì¢…ë£Œì¼ (YYYYMMDD)
    data_dir       : str   â€“ í”„ë¡œì íŠ¸ ë°ì´í„° ë£¨íŠ¸
    max_workers    : int   â€“ ë©€í‹°ìŠ¤ë ˆë“œ ë³‘ë ¬ ìˆ˜ì§‘ ì›Œì»¤ ìˆ˜
    skip_notebooks : list  â€“ ì‹¤í–‰ì„ ê±´ë„ˆë›°ê³  ì‹¶ì€ ë…¸íŠ¸ë¶ íŒŒì¼ëª… ëª©ë¡ (optional)
    """

    skip_notebooks = skip_notebooks or []

    # 0. env & dirs
    load_dotenv(dotenv_path=".env")
    os.makedirs(data_dir, exist_ok=True)

    csv_path      = os.path.join(data_dir, "dividend_with_text.csv")
    jsonl_path    = os.path.join(data_dir, "dividend_with_text.jsonl")
    ml_ready_path = os.path.join(data_dir, "dividend_ml_ready.csv")
    hist_path     = os.path.join(data_dir, "price_history.csv")
    check_path    = os.path.join(data_dir, "window_check_result.csv")
    cache_dir     = os.path.join(data_dir, "price_cache")
    module_dir    = os.path.join(data_dir, "module_datasets")
    results_dir   = os.path.join(data_dir, "results")
    artifacts_dir = os.path.join("artifacts")

    for d in [module_dir, results_dir, artifacts_dir, cache_dir]:
        os.makedirs(d, exist_ok=True)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 1. ì¦ë¶„ ê³µì‹œ ìˆ˜ì§‘
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n1âƒ£  ë°°ë‹¹ê³µì‹œ ì¦ë¶„ ìˆ˜ì§‘")
    new_records = collect_dividend_filings_incremental(
        start=start_date,
        end=end_date,
        save_csv=csv_path,
        save_jsonl=jsonl_path,
        existing_jsonl=jsonl_path,
        max_workers=max_workers,
    )
    print(f"   â–¶ ì‹ ê·œ ìˆ˜ì§‘ ê±´ìˆ˜: {len(new_records):,}ê±´")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 2. ML ë°ì´í„° ì •ì œ â†’ dividend_ml_ready.csv
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n2âƒ£  MLìš© ë°ì´í„° ì •ì œ & ì €ì¥")
    df = pd.read_csv(csv_path, encoding="utf-8-sig")
    print(f"   ì›ë³¸ shape: {df.shape}")
    df = clean_ml_data(df)
    print(f"   ì •ì œ í›„ shape: {df.shape}")
    df.to_csv(ml_ready_path, index=False, encoding="utf-8-sig")
    print(f"   âœ… ì €ì¥ ì™„ë£Œ â†’ {ml_ready_path}")

    # 2-1. Price fetch & window check
    print("\n2.1 ì£¼ê°€ ìˆ˜ì§‘ & ìœˆë„ìš° ê²€ì¦")
    run_price_fetching(
        div_path       = ml_ready_path,
        hist_path      = hist_path,
        check_path     = check_path,
        cache_dir_path = cache_dir,
        window_days    = 30,
        max_workers    = max_workers,
    )

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 3. Feature Engineering (03_feature_splits.ipynb)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n3âƒ£  ê³µí†µ í”¼ì²˜ ìƒì„± & ë°ì´í„° ë¶„í•  (papermill)")
    try:
        pm.execute_notebook(
            input_path  = os.path.join("notebooks", "03_feature_splits.ipynb"),
            output_path = os.path.join(artifacts_dir, "03_feature_splits.out.ipynb"),
            parameters  = {
                "data_dir":   data_dir,
                "out_dir":    module_dir,
                "clf_window":      1,
                "reg_window":     10,
                "cluster_window": 10,
            },
            kernel_name=None,
        )
    except Exception:
        print("   âš ï¸  03_feature_splits.ipynb ì‹¤í–‰ ì‹¤íŒ¨ â€” ìŠ¤íƒíŠ¸ë ˆì´ìŠ¤ ì¶œë ¥")
        traceback.print_exc()
        sys.exit(1)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 4. Embedding & FAISS
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n4âƒ£  ë¬¸ì„œ ì„ë² ë”© & FAISS ìƒ‰ì¸")
    embed_utils.jsonl_to_faiss(
        jsonl_path=jsonl_path,
        faiss_path=os.path.join(data_dir, "dividend_faiss_index"),
    )
    print("   âœ… FAISS ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 5. Notebook-based model training (04-06)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    nb_seq = [
        "04_classification.ipynb",
        "05_regression.ipynb",
        "06_clustering.ipynb",
    ]

    for nb in nb_seq:
        if nb in skip_notebooks:
            print(f"   â­ï¸  {nb}  ê±´ë„ˆëœ€ (skip_notebooks ì„¤ì •) ")
            continue
        print(f"\n5âƒ£  papermill ì‹¤í–‰ â†’ {nb}")
        try:
            pm.execute_notebook(
                input_path  = os.path.join("notebooks", nb),
                output_path = os.path.join(artifacts_dir, nb.replace(".ipynb", ".out.ipynb")),
                parameters  = {
                    "data_dir":  data_dir,
                    "module_dir": module_dir,
                },
            )
        except FileNotFoundError:
            print(f"   âš ï¸  {nb} íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ â€” ê±´ë„ˆëœ€")
        except Exception:
            print(f"   âš ï¸  {nb} ì‹¤í–‰ ì˜¤ë¥˜ â€” ê³„ì† ì§„í–‰")
            traceback.print_exc()

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 6. Ensemble & Master CSV
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    master_csv_path = os.path.join(data_dir, "all_stocks_master.csv")

    if "07_ensemble.ipynb" not in skip_notebooks and os.path.exists(os.path.join("notebooks", "07_ensemble.ipynb")):
        print("\n6âƒ£  papermill ì‹¤í–‰ â†’ 07_ensemble.ipynb")
        try:
            pm.execute_notebook(
                input_path  = os.path.join("notebooks", "07_ensemble.ipynb"),
                output_path = os.path.join(artifacts_dir, "07_ensemble.out.ipynb"),
                parameters  = {
                    "module_dir": module_dir,
                    "data_dir":   data_dir,
                    "master_csv": master_csv_path,
                },
            )
        except Exception:
            print("   âš ï¸  07_ensemble.ipynb ì‹¤í–‰ ì‹¤íŒ¨ â€” ì½”ë“œ fallback ìœ¼ë¡œ ì „í™˜")
            traceback.print_exc()
            _build_master_csv(module_dir, data_dir, master_csv_path)
    else:
        # ë…¸íŠ¸ë¶ ìŠ¤í‚µ ë˜ëŠ” ì—†ìŒ â†’ inline í•¨ìˆ˜ ì´ìš©
        print("\n6âƒ£  Ensemble ë…¸íŠ¸ë¶ ê±´ë„ˆëœ€ â€” inline í•¨ìˆ˜ë¡œ Master CSV ìƒì„±")
        _build_master_csv(module_dir, data_dir, master_csv_path)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 7. Optional: 08_dividend.ipynb í›„ì²˜ë¦¬
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    nb08 = "08_dividend.ipynb"
    if nb08 not in skip_notebooks and os.path.exists(os.path.join("notebooks", nb08)):
        print("\n7âƒ£  papermill ì‹¤í–‰ â†’ 08_dividend.ipynb")
        try:
            pm.execute_notebook(
                input_path  = os.path.join("notebooks", nb08),
                output_path = os.path.join(artifacts_dir, nb08.replace(".ipynb", ".out.ipynb")),
                parameters  = {
                    "data_dir": data_dir,
                    "master_csv": master_csv_path,
                },
            )
        except Exception:
            print("   âš ï¸  08_dividend.ipynb ì‹¤í–‰ ì˜¤ë¥˜ â€” ê³„ì† ì§„í–‰")
            traceback.print_exc()
    else:
        print("   â­ï¸  08_dividend.ipynb  ê±´ë„ˆëœ€")

    print("\nğŸ‰  ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    # ì˜ˆì‹œ: python run_pipeline.py 20130101 20250630 data 8 04_classification.ipynb
    import argparse

    parser = argparse.ArgumentParser(description="Dividend Agent End-to-End Pipeline")
    parser.add_argument("--start",  type=str, default="20130101", help="ì‹œì‘ì¼ (YYYYMMDD)")
    parser.add_argument("--end",    type=str, default=datetime.today().strftime("%Y%m%d"), help="ì¢…ë£Œì¼ (YYYYMMDD)")
    parser.add_argument("--data",   type=str, default="data", help="ë°ì´í„° ë””ë ‰í† ë¦¬")
    parser.add_argument("--workers",type=int, default=10, help="max_workers")
    parser.add_argument("--skip",   nargs="*", default=[], help="ê±´ë„ˆë›¸ ë…¸íŠ¸ë¶ íŒŒì¼ëª… ëª©ë¡")
    args = parser.parse_args()

    run_pipeline(
        start_date=args.start,
        end_date=args.end,
        data_dir=args.data,
        max_workers=args.workers,
        skip_notebooks=args.skip,
    )