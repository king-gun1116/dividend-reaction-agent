"""
run_pipeline.py (rev 2025-07-05b)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
End-to-End ë°°ë‹¹ ê³µì‹œ Agent íŒŒì´í”„ë¼ì¸

â— Step 1  â”€ ì¦ë¶„ ê³µì‹œ ìˆ˜ì§‘  (utils.dart_api.collect_dividend_filings_incremental)
â— Step 2  â”€ ML í•™ìŠµìš© CSV ì •ì œ   â†’ dividend_ml_ready.csv
â— Step 3  â”€ í…ìŠ¤íŠ¸ ì„ë² ë”© & FAISS ì¸ë±ìŠ¤ â†’ dividend_faiss_index/

ë³€ê²½ì 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. ìƒˆ dart_api ë¡œì§ ì ìš© (corp_code ìºì‹± / document.xml ìš°ì„ )
2. start_date Â· end_date ê¸°ë³¸ê°’ì„ ìµœê·¼ 3ë…„ìœ¼ë¡œ ë‹¨ì¶• (ì†ë„ ê°œì„ )
3. max_workers íŒŒë¼ë¯¸í„° ë…¸ì¶œ (ë””í´íŠ¸ 10)
4. ì§„í–‰ ë¡œê·¸ë¥¼ tqdm + rich.pretty ë¡œ ê°€ë…ì„± í–¥ìƒ (ì„ íƒ)
"""

from __future__ import annotations

import os
import pandas as pd
from dotenv import load_dotenv

# ë‚´ë¶€ ëª¨ë“ˆ
from utils.dart_api import collect_dividend_filings_incremental
from utils import embed_utils


def run_pipeline(
    start_date: str = "20130101",  
    end_date:   str = "20250630",
    data_dir:   str = "data",
    max_workers: int = 10,
) -> None:
    """ë°°ë‹¹ ê³µì‹œ Agent ì „ì²´ íŒŒì´í”„ë¼ì¸"""

    # 0) í™˜ê²½ë³€ìˆ˜ ë¡œë“œ (DART_API_KEY ë“±)
    load_dotenv(dotenv_path=".env")
    os.makedirs(data_dir, exist_ok=True)

    # â”€â”€ ê²½ë¡œ ì •ì˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    csv_path        = os.path.join(data_dir, "dividend_with_text.csv")
    jsonl_path      = os.path.join(data_dir, "dividend_with_text.jsonl")
    ml_ready_path   = os.path.join(data_dir, "dividend_ml_ready.csv")
    faiss_index_dir = os.path.join(data_dir, "dividend_faiss_index")

    # 1ï¸âƒ£ ì¦ë¶„ ìˆ˜ì§‘
    print("\n1ï¸âƒ£  ë°°ë‹¹ê³µì‹œ ì¦ë¶„ ìˆ˜ì§‘ (ì´ë¯¸ ìˆ˜ì§‘ëœ ê±´ì€ ê±´ë„ˆëœ€)")
    new_records = collect_dividend_filings_incremental(
        start=start_date,
        end=end_date,
        save_csv=csv_path,
        save_jsonl=jsonl_path,
        existing_jsonl=jsonl_path,
        max_workers=max_workers,
    )
    print(f"   â–¶ ì‹ ê·œ ìˆ˜ì§‘ ê±´ìˆ˜: {len(new_records):,} ê±´")

    # 2ï¸âƒ£ ML í•™ìŠµìš© CSV ì •ì œ
    print("\n2ï¸âƒ£  MLìš© ë°ì´í„° ì¤€ë¹„")
    df = pd.read_csv(csv_path, encoding="utf-8-sig")

    # â”€â”€ 2-1) HTML ì›ë¬¸ ì œê±°
    if "html" in df.columns:
        df = df.drop(columns=["html"])

    # â”€â”€ 2-2) ê²°ì¸¡ì¹˜ê°€ ì§€ë‚˜ì¹˜ê²Œ ë§ì€ ì»¬ëŸ¼ ë“œë¡­
    drop_cols = [
        "div_type", "div_kind", "per_share_preferred", "yield_preferred",
    ]
    df = df.drop(columns=[c for c in drop_cols if c in df.columns])

    # â”€â”€ 2-3) í•„ìˆ˜ ì»¬ëŸ¼ ê²°ì¸¡ ì‚­ì œ
    df = df.dropna(subset=["per_share_common", "total_amount"]).reset_index(drop=True)

    # â”€â”€ 2-4) ì¤‘ì•™ê°’ ëŒ€ì²´ (yield_common)
    if "yield_common" in df.columns:
        df["yield_common"] = df["yield_common"].fillna(df["yield_common"].median())

    # â”€â”€ 2-5) ë¬¸ìì—´ â†’ ì‹¤ìˆ˜ ë³€í™˜ (ì½¤ë§ˆ ì œê±°)
    numeric_cols = ["per_share_common", "yield_common", "total_amount"]
    for col in numeric_cols:
        if col in df.columns:
            df[col] = (
                df[col].astype(str).str.replace(",", "", regex=False).astype(float)
            )

    df.to_csv(ml_ready_path, index=False, encoding="utf-8-sig")
    print(f"âœ…  MLìš© ë°ì´í„° ì €ì¥ â†’ {ml_ready_path}")

    # 3ï¸âƒ£ í…ìŠ¤íŠ¸ ì„ë² ë”© & FAISS ì¸ë±ìŠ¤
    print("\n3ï¸âƒ£  Agent ìƒ‰ì¸ (ì„ë² ë”© ë° FAISS ì¸ë±ì‹±)")
    embed_utils.jsonl_to_faiss(
        jsonl_path=jsonl_path,
        faiss_path=faiss_index_dir,
    )
    print(f"âœ…  FAISS ì¸ë±ìŠ¤ ì €ì¥ â†’ {faiss_index_dir}")

    print("\nğŸ‰  ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ")


if __name__ == "__main__":
    run_pipeline()